import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import re
from matplotlib.backends.backend_pdf import PdfPages

def plot_best_run_scaling_bars(df: pd.DataFrame, metric_col: str, title: str, ax: plt.Axes):
    sns.barplot(data=df, x='Nodes', y=metric_col, hue='density', hue_order=['Sparse', 'Medium', 'Dense'],
                palette='colorblind', ax=ax, errorbar=None)

    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel('Number of Nodes', fontsize=10)
    ax.set_ylabel(metric_col.replace('_', ' ').replace('diff', 'Error').title(), fontsize=10) # Simplified Y-axis label
    ax.legend(title='Density', fontsize=8)
    ax.grid(True, axis='y', linestyle='--', alpha=0.7)
    
    # Add qubit count to x-tick labels
    try:
        labels = [item.get_text() for item in ax.get_xticklabels()]
        new_labels = [f'{n} ({int(int(n)*(int(n)-1)/2)} qubits)' for n in labels]
        ax.set_xticklabels(new_labels)
    except (ValueError, IndexError):
        # In case labels are not purely numeric, we avoid crashing.
        print(f"Warning: Could not format x-tick labels for '{title}' bar plot.")

def plot_best_run_scaling_lines(df: pd.DataFrame, metric_col: str, title: str, ax: plt.Axes):
    bipartite_df = df[df['Dataset'] == 'Bipartite']
    er_df = df[df['Dataset'] == 'ER']

    if not bipartite_df.empty:
        sns.lineplot(data=bipartite_df, x='Nodes', y=metric_col, hue='density',
                     hue_order=['Sparse', 'Medium', 'Dense'], palette='colorblind',
                     style=True, dashes=[(1,0)]*3, markers=True,
                     lw=2.5, ax=ax, markersize=8, errorbar=None, legend='full')
    if not er_df.empty:
        sns.lineplot(data=er_df, x='Nodes', y=metric_col, hue='density',
                     hue_order=['Sparse', 'Medium', 'Dense'], palette='colorblind',
                     style=True, dashes=[(4,1)]*3, markers=True,
                     lw=2.5, ax=ax, markersize=8, errorbar=None, legend=False)

    ax.set_title(title, fontsize=14, fontweight='bold')
    # --- MODIFIED: Updated X and Y axis labels ---
    ax.set_xlabel('Number of Nodes', fontsize=10)
    ax.set_ylabel(metric_col.replace('_', ' ').replace('diff', 'Error').title(), fontsize=10) # Simplified Y-axis label

    handles, labels = ax.get_legend_handles_labels()
    # Correctly slice legend to only show density types
    if handles and labels:
        density_handles = [h for h, l in zip(handles, labels) if l in ['Sparse', 'Medium', 'Dense']]
        density_labels = [l for l in labels if l in ['Sparse', 'Medium', 'Dense']]
        if density_handles:
             ax.legend(density_handles, density_labels, title='Density', fontsize=8)
        else:
             ax.legend(title='Density', fontsize=8)

    ax.grid(True, linestyle='--', alpha=0.7)
    
    # --- MODIFIED: Add qubit count to x-tick labels ---
    nodes = sorted(df['Nodes'].unique())
    ax.set_xticks(nodes)
    ax.set_xticklabels([f'{n} ({int(n*(n-1)/2)} qubits)' for n in nodes], rotation=45, ha='right')

def plot_evaluation_vs_dataset(df: pd.DataFrame, metric_base_name: str, ax: plt.Axes):
    gen_col = f'generated_{metric_base_name}'
    data_col = f'dataset_{metric_base_name}'
    if gen_col not in df.columns or data_col not in df.columns:
        ax.text(0.5, 0.5, f"Data for '{metric_base_name}'\nnot found.", ha='center', va='center')
        ax.set_title(f"{metric_base_name.replace('_', ' ').title()}", fontweight='bold')
        return
    sns.scatterplot(data=df, x=data_col, y=gen_col, hue='Nodes', style='Dataset',s=120, alpha=0.9, ax=ax, palette='Dark2')
    min_val = min(df[data_col].min(), df[gen_col].min()) * 0.95
    max_val = max(df[data_col].max(), df[gen_col].max()) * 1.05
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')
    ax.set_title(f"Generated vs. Ground Truth: {metric_base_name.replace('_', ' ').title()}", fontweight='bold')
    ax.set_xlabel("Dataset Ground Truth", fontsize=10)
    # --- MODIFIED: Replaced 'Evaluation' with 'NISQ' ---
    ax.set_ylabel("Generated by Best Model (NISQ)", fontsize=10)
    ax.legend(fontsize=8, bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.grid(True)

def plot_evaluation_diff_distribution(df: pd.DataFrame, diff_col: str, ax: plt.Axes):
    clean_name = diff_col.replace('_', ' ').replace('pct', 'Percentage').replace('diff', 'Difference').title()
    title = f"Distribution of Absolute Error\nfor {clean_name}"
    sns.kdeplot(data=df, x=diff_col, fill=True, alpha=0.6, ax=ax, color='navy')
    ax.set_title(title, fontweight='bold')
    ax.set_xlabel("Absolute Difference from Ground Truth", fontsize=10)
    ax.set_ylabel("Density", fontsize=10)
    ax.grid(True)

def plot_simulation_vs_evaluation_8N_all(df: pd.DataFrame, metric_col: str, ax: plt.Axes):
    df_8n = df[df['num_nodes'] == 8].copy()
    
    # Pivot on the common_run_key for accurate pairing
    pivot_df = df_8n.pivot_table(
        index=['common_run_key', 'density'], columns='run_type', values=metric_col
    ).reset_index().dropna(subset=['Simulation', 'Evaluation'])

    if pivot_df.empty:
        ax.text(0.5, 0.5, "Not enough paired data for\n8-Node Sim vs. NISQ comparison", ha='center', va='center')
        ax.set_title(f"8-Node Simulation vs. NISQ", fontweight='bold')
        return
        
    sns.scatterplot(data=pivot_df, x='Simulation', y='Evaluation', hue='density',hue_order=['Sparse', 'Medium', 'Dense'], style='density',s=120, alpha=0.9, ax=ax, palette='colorblind')
    min_val = min(pivot_df['Simulation'].min(), pivot_df['Evaluation'].min()) * 0.95
    max_val = max(pivot_df['Simulation'].max(), pivot_df['Evaluation'].max()) * 1.05
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Ideal')
    clean_metric_name = metric_col.replace('_', ' ').replace('gen ', '').replace('pct', 'Percentage').replace('diff', 'Difference').title()
    ax.set_title(f"8-Node Sim vs. NISQ: {clean_metric_name}", fontweight='bold')
    ax.set_xlabel("Simulation Result", fontsize=10)
    ax.set_ylabel("NISQ Result", fontsize=10)
    ax.legend(title='Density', fontsize=8)
    ax.grid(True)

def plot_er_bipartite_generation(df: pd.DataFrame, ax: plt.Axes):
    er_df = df[(df['run_type'] == 'Evaluation') & (df['graph_type'] == 'ER')].copy()
    if er_df.empty:
        ax.text(0.5, 0.5, "No data found for ER models.", ha='center', va='center')
        return
    sns.barplot(data=er_df, x='grouping_key', y='generated_bipartite_pct', palette='viridis', ax=ax, order=sorted(er_df['grouping_key'].unique()))
    ax.set_title("Bipartite Graphs Generated by ER Models (NISQ Runs)", fontsize=16, fontweight='bold')
    ax.set_xlabel("ER Model Type", fontsize=12)
    ax.set_ylabel("Avg. Generated Bipartite %", fontsize=12)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    ax.grid(True, axis='y', linestyle='--', alpha=0.7)


def main(args: argparse.Namespace):
    if not args.summary_file.is_file():
        print(f"Error: Summary file not found at '{args.summary_file}'")
        return

    print(f"Loading data from '{args.summary_file}'...")
    df = pd.read_csv(args.summary_file)
    
    try:
        # Using a more robust LaTeX font setting
        plt.rcParams.update({
            "text.usetex": True,
            "font.family": "serif",
            "font.serif": ["Computer Modern Roman", "Times New Roman"],
        })
        print("Successfully configured Matplotlib to use LaTeX fonts.")
    except Exception as e:
        print(f"Warning: Could not set LaTeX font. Using default. Error: {e}")

    # --- Data Filtering and Preprocessing ---
    df = df[df['num_nodes'] != 14].copy()
    print(f"Removed 14N data. Remaining entries: {len(df)}")
    
    df.rename(columns={
        'gen_avg_clustering_coeff': 'generated_avg_clustering_coeff',
        'gen_avg_shortest_path': 'generated_avg_shortest_path',
        'clustering_coeff_diff': 'generated_avg_clustering_coeff_diff',
        'avg_shortest_path_diff': 'generated_avg_shortest_path_diff'
    }, inplace=True)
    
    for col in df.columns:
        if col.endswith('_diff'):
            df[col] = df[col].abs()
            
    if 'bipartite_pct_diff' not in df.columns:
        df['bipartite_pct_diff'] = (df['generated_bipartite_pct'] - df['dataset_bipartite_pct']).abs()

    def get_density(name_str):
        if 'Sparse' in name_str: return 'Sparse'
        if 'Medium' in name_str: return 'Medium'
        if 'Dense' in name_str: return 'Dense'
        return 'N/A'

    def get_graph_type(name_str):
        if 'Bipartite' in name_str: return 'Bipartite'
        if 'ER' in name_str: return 'ER'
        return 'Unknown'

    df['density'] = df['dataset_name'].apply(get_density)
    df['graph_type'] = df['dataset_name'].apply(get_graph_type)
    df['grouping_key'] = df['num_nodes'].astype(str) + 'N_' + df['graph_type'] + '_' + df['density']
    
    # Create a common key for pairing sim/eval runs
    df['common_run_key'] = df['run_id'].str.replace('simulation_results/', '', regex=False).str.replace('evaluation_results/', '', regex=False)
    
    eval_df = df[df['run_type'] == 'Evaluation'].copy()
    
    idx_edge_prob = eval_df.groupby('grouping_key')['edge_prob_diff'].idxmin()
    best_by_edge_prob = eval_df.loc[idx_edge_prob].copy()
    best_by_edge_prob.rename(columns={'num_nodes': 'Nodes'}, inplace=True)
    best_by_edge_prob['Dataset'] = best_by_edge_prob['grouping_key'].apply(lambda x: 'Bipartite' if 'Bipartite' in x else 'ER')

    idx_bipartite = eval_df.groupby('grouping_key')['generated_bipartite_pct'].idxmax()
    best_by_bipartite_pct = eval_df.loc[idx_bipartite].copy()
    best_by_bipartite_pct.rename(columns={'num_nodes': 'Nodes'}, inplace=True)
    best_by_bipartite_pct['Dataset'] = best_by_bipartite_pct['grouping_key'].apply(lambda x: 'Bipartite' if 'Bipartite' in x else 'ER')

    output_path = args.summary_file.parent / 'performance_report.pdf'
    
    with PdfPages(output_path) as pdf:
        # The plotting pages remain the same, but functions are now modified
        for plot_func, plot_type in zip([plot_best_run_scaling_bars, plot_best_run_scaling_lines], ["Bar Plots", "Line Plots"]):
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle(f"Best Run Performance (by Edge Probability Error) - {plot_type}", fontsize=20, fontweight='bold')
            plot_func(best_by_edge_prob[best_by_edge_prob['Dataset'] == 'Bipartite'], 'generated_bipartite_pct', 'Bipartite Percentage', axes[0, 0])
            plot_func(best_by_edge_prob, 'edge_prob_diff', 'Edge Probability Absolute Error', axes[0, 1])
            plot_func(best_by_edge_prob, 'generated_avg_clustering_coeff_diff', 'Clustering Coefficient Error', axes[1, 0])
            plot_func(best_by_edge_prob, 'generated_avg_shortest_path_diff', 'Avg. Shortest Path Error', axes[1, 1])
            plt.tight_layout(rect=[0, 0.03, 1, 0.96])
            pdf.savefig(fig)
            plt.close()

        for plot_func, plot_type in zip([plot_best_run_scaling_bars, plot_best_run_scaling_lines], ["Bar Plots", "Line Plots"]):
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle(f"Best Run Performance (by Bipartite % Accuracy) - {plot_type}", fontsize=20, fontweight='bold')
            plot_func(best_by_bipartite_pct[best_by_bipartite_pct['Dataset'] == 'Bipartite'], 'generated_bipartite_pct', 'Bipartite Percentage', axes[0, 0])
            plot_func(best_by_bipartite_pct, 'edge_prob_diff', 'Edge Probability Absolute Error', axes[0, 1])
            plot_func(best_by_bipartite_pct, 'generated_avg_clustering_coeff_diff', 'Clustering Coefficient Error', axes[1, 0])
            plot_func(best_by_bipartite_pct, 'generated_avg_shortest_path_diff', 'Avg. Shortest Path Error', axes[1, 1])
            plt.tight_layout(rect=[0, 0.03, 1, 0.96])
            pdf.savefig(fig)
            plt.close()
        
        fig, axes = plt.subplots(1, 2, figsize=(18, 7))
        # --- MODIFIED: Replaced 'Evaluation' with 'NISQ' ---
        fig.suptitle(r"NISQ vs. Dataset (Best Runs by Edge Prob. Error)", fontsize=20, fontweight='bold')
        plot_evaluation_vs_dataset(best_by_edge_prob, 'edge_prob', axes[0])
        plot_evaluation_vs_dataset(best_by_edge_prob, 'mean_degree', axes[1])
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        pdf.savefig(fig)
        plt.close()

        fig, axes = plt.subplots(1, 2, figsize=(18, 7))
        # --- MODIFIED: Replaced 'Evaluation' with 'NISQ' ---
        fig.suptitle(r"NISQ vs. Dataset (Best Runs by Bipartite % Accuracy)", fontsize=20, fontweight='bold')
        plot_evaluation_vs_dataset(best_by_bipartite_pct, 'edge_prob', axes[0])
        plot_evaluation_vs_dataset(best_by_bipartite_pct, 'mean_degree', axes[1])
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        pdf.savefig(fig)
        plt.close()

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        # --- MODIFIED: Replaced 'Evaluation' with 'NISQ' ---
        fig.suptitle(r"Distribution of Absolute Errors (All NISQ Runs)", fontsize=20, fontweight='bold')
        plot_evaluation_diff_distribution(eval_df, 'edge_prob_diff', axes[0, 0])
        plot_evaluation_diff_distribution(eval_df, 'bipartite_pct_diff', axes[0, 1])
        plot_evaluation_diff_distribution(eval_df, 'generated_avg_clustering_coeff_diff', axes[1, 0])
        plot_evaluation_diff_distribution(eval_df, 'generated_avg_shortest_path_diff', axes[1, 1])
        plt.tight_layout(rect=[0, 0.03, 1, 0.96])
        pdf.savefig(fig)
        plt.close()

        fig, axes = plt.subplots(2, 2, figsize=(18, 14))
        # --- MODIFIED: Replaced 'Hardware Evaluation' with 'NISQ' ---
        fig.suptitle(r"8-Node Graphs: Simulation vs. NISQ (All Runs)", fontsize=20, fontweight='bold')
        plot_simulation_vs_evaluation_8N_all(df, 'generated_edge_prob', axes[0, 0])
        plot_simulation_vs_evaluation_8N_all(df, 'generated_avg_clustering_coeff', axes[0, 1])
        plot_simulation_vs_evaluation_8N_all(df, 'bipartite_pct_diff', axes[1, 0])
        plot_simulation_vs_evaluation_8N_all(df, 'generated_avg_shortest_path_diff', axes[1, 1])
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        pdf.savefig(fig)
        plt.close()
        
        fig, ax = plt.subplots(figsize=(14, 8))
        plot_er_bipartite_generation(df, ax)
        plt.tight_layout()
        pdf.savefig(fig)
        plt.close()

    print(f"\nâœ… Comprehensive PDF report saved successfully to '{output_path}'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate a multi-page PDF report from the master analysis summary CSV."
    )
    parser.add_argument(
        "--summary-file",
        type=Path,
        default=Path("results/master_analysis_summary_with_run_id.csv"),
        help="Path to the master analysis summary CSV file."
    )
    args = parser.parse_args()
    main(args)