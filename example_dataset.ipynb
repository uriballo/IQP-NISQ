{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "from datasets.mnist import MNISTDataset\n",
    "from src.autoencoders.simple_vae import model\n",
    "from src.utils.autoencoder_trainer import AutoencoderTrainer\n",
    "from src.utils.autoencoder_manager import get_latent_dataset, restore_model_state\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.PRNGKey(0)\n",
    "binary_vae = model(latents=20)\n",
    "input_shape = (64, 196)\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Load MNIST dataset for training and testing\n",
    "train_dataset = MNISTDataset(split='train', batch_size=64, image_size=(14, 14)).load()\n",
    "test_dataset = MNISTDataset(split='test', batch_size=64, image_size=(14, 14)).load()\n",
    "\n",
    "# Create an instance of the trainer with your binary VAE model\n",
    "trainer = AutoencoderTrainer(binary_vae, learning_rate, rng, input_shape)\n",
    "state = trainer.state\n",
    "params = trainer.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restored from directory '/Users/uribagi/Documents/GitHub/Latent-IQP/weights/binary_vae_500epoch_3e-4lr/checkpoint_461'.\n",
      "Weights loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uribagi/Documents/GitHub/Latent-IQP/.venv/lib/python3.12/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1250: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"weights/binary_vae_500epoch_3e-4lr/checkpoint_461\"\n",
    "state = restore_model_state(ckpt_dir, state)\n",
    "\n",
    "print(\"Weights loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latents, train_labels = get_latent_dataset(binary_vae, state.params, tfds.as_numpy(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train_latents[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(train_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_latents, test_labels = get_latent_dataset(binary_vae, state.params, tfds.as_numpy(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(test_latents[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(test_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(\"datasets/mnist_latent_train.npy\", train_latents)\n",
    "np.save(\"datasets/mnist_labels_train.npy\", train_labels)\n",
    "\n",
    "np.save(\"datasets/mnist_latent_test.npy\", test_latents)\n",
    "np.save(\"datasets/mnist_labels_test.npy\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.latent_mnist import LatentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a saved dataset\n",
    "latent_file = './datasets/mnist_latent_train.npy'\n",
    "label_file = './datasets/mnist_labels_train.npy'\n",
    "dataset = LatentDataset(latent_file, label_file, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=(TensorSpec(shape=(None, 20), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(8, 20), dtype=int64, numpy=\n",
      "array([[0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "       [1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
      "       [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0],\n",
      "       [0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
      "       [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0]])>, <tf.Tensor: shape=(8,), dtype=int64, numpy=array([0, 5, 9, 7, 4, 3, 5, 3])>)\n"
     ]
    }
   ],
   "source": [
    "for d in ds:\n",
    "    print(d)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
